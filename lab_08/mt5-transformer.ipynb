{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# on colab, you will have to install transformers and sentencepiece\n","%%bash\n","pip install transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","import random\n","\n","# huggingface\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# data logistics: load theses title and abstract\n","# limit_title_len=[4,10] restricts to titles in between 4 and 10 tokens\n","def load_thesis_data(path='res/theses.tsv', limit_title_len=None):\n","    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n","    # limit to 'Sprache'==DE and title_len if necessary\n","    df = pd.read_csv(path, sep='\\t')\n","\n","    if limit_title_len:\n","        df['title_length'] = df['Titel'].apply(lambda x: len(x.split()))\n","        df = df[df['title_length'].between(limit_title_len[0], limit_title_len[1])] # pick your numbers or use std dev\n","    \n","    return df[df['Sprache'] == 'DE']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set up the models; they will download on first time use but this will take some time (1.2 GB)\n","# https://huggingface.co/transformers/model_doc/auto.html?highlight=autotokenizer#transformers.AutoTokenizer.from_pretrained\n","# https://huggingface.co/transformers/model_doc/auto.html?highlight=autotokenizer#transformers.AutoModelForSeq2SeqLM.from_pretrained\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# method for summary generation, using the global model and tokenizer\n","def generate_summary(model, abstract, num_beams, repetition_penalty,\n","                    length_penalty, early_stopping, max_output_length):\n","    # source_encoding = tokenizer(...)\n","    source_encoding = tokenizer(\n","        abstract,\n","        max_length=784,  # this comes from the\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_attention_mask=True,\n","        add_special_tokens=True,\n","        return_tensors=\"pt\")\n","\n","    # generated_ids = model.generate(...)\n","    generated_ids = model.generate(\n","        input_ids=source_encoding[\"input_ids\"],\n","        attention_mask=source_encoding[\"attention_mask\"],\n","        num_beams=num_beams,\n","        max_length=max_output_length,\n","        repetition_penalty=repetition_penalty,\n","        length_penalty=length_penalty,\n","        early_stopping=early_stopping,\n","        use_cache=True)\n","\n","    # ...map to string using tokenizer.decode and return\n","    preds = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n","         for gen_id in generated_ids]\n","\n","    return \"\".join(preds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# main program\n","df = load_thesis_data()\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# now use the pre-trained model to generate some short summaries from the \n","# abstracts, and compare them to the reference titles\n","\n","# adjust these values as desired\n","num_beams = 2\n","repetition_penalty = 1.0\n","length_penalty = 2.0\n","max_output_length = 120\n","\n","early_stopping = True\n","\n","# sample from dataset, using abstracts as input to generate short summary (~title)\n","from IPython.display import HTML, display\n","def displaysum(summarize, generated, reference):\n","    display(HTML(f\"\"\"<table>\n","    <tr><td>summarize:</td><td>{summarize}</td></tr>\n","    <tr><td>generated:</td><td>{generated}</td></tr>\n","    <tr><td>reference:</td><td>{reference}</td></tr>\n","    </table>\n","    \"\"\"))\n","\n","for i in [random.randint(0, len(df) - 1) for _ in range(10)]:\n","    # load the values\n","    summarize = df.iloc[i].Abstract\n","    reference = df.iloc[i].Titel\n","\n","    # generated = generate_summary(...)\n","    generated = generate_summary(model, \"summarize: \" + str(df.iloc[i].Abstract),\n","                                 num_beams, repetition_penalty,\n","                                 length_penalty, early_stopping, 120)\n","\n","    displaysum(summarize, generated, reference)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Task 2: Fine-tuning\n","\n","# As you could see, the summary quality is pretty much hit-or-miss. Let's use\n","# a good share of the data to fine-tune the pre-trained model to our task.\n","\n","from torch.utils.data import Dataset\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ThesisDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_input_len, max_output_len):\n","        self.tokenizer = tokenizer\n","        self.source_len = max_input_len\n","        self.summ_len = max_output_len\n","        self.Titel = df.Titel\n","\n","        # T5 requires us to prepend the task\n","        self.Abstract = 'summarize: ' + df.Abstract\n","\n","    def __len__(self):\n","        return len(self.Titel)\n","\n","    def __getitem__(self, index):\n","        abstract = str(self.Abstract[index])\n","        title = str(self.Titel[index])\n","\n","        # use tokenizer.batch_encode_plus to also get the masking\n","        source_tok = self.tokenizer.batch_encode_plus([abstract],\n","                                                      max_length=self.source_len, \n","                                                      truncation=True,\n","                                                      pad_to_max_length=True,\n","                                                      return_tensors='pt')\n","        label_tok = self.tokenizer.batch_encode_plus([title],\n","                                                     max_length=self.summ_len,\n","                                                     truncation=True,\n","                                                     pad_to_max_length=True,\n","                                                     return_tensors='pt')\n","\n","        input_ids = source_tok['input_ids'].squeeze()\n","        input_mask = source_tok['attention_mask'].squeeze()\n","        label_ids = label_tok['input_ids'].squeeze()\n","        label_mask = label_tok['attention_mask'].squeeze()\n","\n","        return {\n","            'input_ids': input_ids.to(dtype=torch.long), \n","            'input_mask': input_mask.to(dtype=torch.long), \n","            'label_ids': label_ids.to(dtype=torch.long),\n","            'label_mask': label_mask.to(dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# for each point in the data loader, compute the forward pass, loss and\n","# backward pass\n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","\n","    for i, data in enumerate(loader, 0):\n","        y = data['label_ids'].to(device, dtype=torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","\n","        # set the padding symbols to -100 to be ignored by torch\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","\n","        inputs = data['input_ids'].to(device, dtype=torch.long)\n","        mask = data['input_mask'].to(device, dtype=torch.long)\n","\n","        # compute forward pass\n","        outputs = model(input_ids=inputs, attention_mask=mask,\n","                        decoder_input_ids=y_ids, labels=lm_labels)\n","\n","        loss = outputs[0]\n","\n","        if i % 10 == 0:\n","            print({\"Training Loss\": loss.item()})\n","\n","        # reset optimizer, do backwards pass and optimizer step    \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# for validation, set the model to eval mode and compute all predictions\n","def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for i, data in enumerate(loader, 0):\n","            y = data['label_ids'].to(device, dtype=torch.long)\n","            ids = data['input_ids'].to(device, dtype=torch.long)\n","            mask = data['input_mask'].to(device, dtype=torch.long)\n","\n","            # make prediction\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                max_length=150,\n","                num_beams=2,\n","                repetition_penalty=2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","                )\n","            \n","            # use tokenizer.decode to get predicted and target string\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            \n","            if i % 100 == 0:\n","                print(f'Completed {i}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    \n","    return predictions, actuals\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# defining some parameters that will be used later on in the training  \n","batch_size_train = 32\n","batch_size_vali = 4\n","\n","max_input_len = 786    # 512?\n","max_output_len = 120\n","\n","# set random seeds and deterministic pytorch for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","# TODO verify: tokenizer and df still loaded and current?\n","\n","# split the dataframe into training and validation\n","df_train = df.sample(frac=0.8, random_state=seed)\n","df_vali = df.drop(df_train.index).reset_index(drop=True)\n","df_train = df_train.reset_index(drop=True)\n","print(f\"df={df.shape}, train={df_train.shape}, vali={df_vali.shape}\")\n","\n","# Creating the Training and Validation dataset for further creation of Dataloader\n","ds_train = ThesisDataset(df_train, tokenizer, max_input_len, max_output_len)\n","ds_vali = ThesisDataset(df_vali, tokenizer, max_input_len, max_output_len)\n","\n","# create data loaders for training and validation\n","from torch.utils.data import DataLoader\n","dl_train = DataLoader(ds_train, shuffle=True, num_workers=0, batch_size=batch_size_train)\n","dl_vali = DataLoader(ds_vali, shuffle=True, num_workers=0, batch_size=batch_size_vali)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# we'll start from the same ml6team/mt5-small-german-finetune-mlsum that we\n","# used before in our baseline experiment; we will reload it below so that we\n","# maintain the base model\n","base = model\n","\n","# this time, we'll load it explicitly as a T5ForConditionalGeneration; the\n","# tokenizer will be the same\n","\n","from transformers import T5ForConditionalGeneration\n","model = T5ForConditionalGeneration.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n","model = model.to(device)\n","\n","# Defining the optimizer that will be used to tune the weights of the network in the training session. \n","optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs_train = 3\n","epochs_vali = [1, 2, 3]\n","\n","models = []\n","\n","for epoch in range(epochs_train):\n","    # call the training routine from above\n","    train(epoch, tokenizer, model, device, dl_train, optimizer)\n","\n","    if epoch in epochs_vali:\n","        # call the vali routine from above to generate some summaries\n","        predictions, actuals = validate(epoch, tokenizer, model, device, dl_vali)\n","\n","        # display some...\n","        for i in [random.randint(0, len(predictions) - 1) for _ in range(10)]:\n","            displaysum(None, generated, reference)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load model and compare outputs\n","base = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n","fine = model  # or any other checkpoint from res/mt5-small-fine-tune-...\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pick some random theses and compare the two models\n","thesis_picks = [random.randint(0, len(df_vali) - 1) for _ in range(10)]\n","for num, i in enumerate(thesis_picks):\n","    print()\n","    # generate a summary with each of the models\n","    s0 = generate_summary(base, 'summarize: '+df_vali.iloc[i].Abstract,\n","                          num_beams, repetition_penalty,\n","                          length_penalty, early_stopping, max_output_len)\n","    s1 = generate_summary(base, ('summarize: '+df_vali.iloc[i].Abstract).lower(),\n","                          num_beams, repetition_penalty,\n","                          length_penalty, early_stopping, max_output_len)\n","    s2 = generate_summary(fine, ('summarize: '+df_vali.iloc[i].Abstract).lower(),\n","                          num_beams, repetition_penalty,\n","                          length_penalty, early_stopping, max_output_len)\n","    display(HTML(f\"\"\"<table>\n","    <tr><td>summarize:</td><td>{df_vali.iloc[i].Abstract}</td></tr>\n","    <tr><td>base:</td><td>{s0}</td></tr>\n","    <tr><td>base (lc):</td><td>{s1}</td></tr>\n","    <tr><td>fine:</td><td>{s2}</td></tr>\n","    <tr><td>reference:</td><td>{df_vali.iloc[i].Titel}</td></tr>\n","    </table>\n","    \"\"\"))\n"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}