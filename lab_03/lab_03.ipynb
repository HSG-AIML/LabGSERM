{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIziVsRIO6d1"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGwNwDKEt8lG"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "##  Lab 03 - \"Supervised Machine Learning\"\n",
    "\n",
    "GSERM'21 course \"Deep Learning: Fundamentals and Applications\", University of St. Gallen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjUA7XZyh7DI"
   },
   "source": [
    "The lab environment of the \"Deep Learning: Fundamentals and Applications\" GSERM course at the University of St. Gallen (HSG) is based on Jupyter Notebooks (https://jupyter.org), which allow to perform a variety of statistical evaluations and data analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYpS4wEPt8lI"
   },
   "source": [
    "In the first labs, you learned about several Python programming elements such as conditions, loops as well as how to implement functions etc. In this third lab, we will build our first **supervised machine learning classification \"pipelines\"**. We will first use a classifier named the Gaussian **Naive-Bayes (NB)** classifier (generative), and then use **Support Vector Machine (SVM)** classification (discriminative).\n",
    "\n",
    "The *generative* **Naive-Bayes (NB)** classifier belongs to the family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with a strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s and remains an accessible (baseline) method for text categorization as well as other domains.\n",
    "\n",
    "The *discriminative* **Support Vector Machine (SVM)** classifier is a supervised machine learning model that learns an optimal separating $n$-dimensional hyperplane to distinguish different observations of training data according to their corresponding class labels. Until recently (before to the advent of deep learning approaches) SVMs have been used in a variety of applications such as isolated handwritten digit recognition[2], object recognition[3], speaker identification[4], face detection in images[5], and text categorisation[6].\n",
    "\n",
    "**Generative** classifiers can be distinguished from **discriminative** classifiers, as shown by the following illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMdudNYut8lJ"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 800px; height: auto\" src=\"classifiers.png\">\n",
    "\n",
    "(Courtesy: Prof. Dr. Borth, University of St. Gallen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Br5f8mEt8lK"
   },
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Jnx-Ljt8lK"
   },
   "source": [
    "## 1. Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybF-i5mQt8lL"
   },
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Know how to setup a **notebook or \"pipeline\"** that solves a simple supervised classification task.\n",
    "> 2. Recognize the **data elements** needed to train and evaluate a supervised machine learning classifier. \n",
    "> 3. Understand how a Gaussian **Naive-Bayes (NB)** classifier can be trained and evaluated.\n",
    "> 4. Understand how a **Suppport Vector Machine (SVM)** classifier can be trained and evaluated.\n",
    "> 5. Train and evaluate **machine learning models** using Python's `scikit-learn` library.\n",
    "> 6. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svq48yIQt8lM"
   },
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyE587hOt8lN"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# OpenAI: \"Solving Rubik's Cube with a Robot Hand\"\n",
    "# YouTubeVideo('x4O8pojMF0w', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZaa0qAnt8lY"
   },
   "source": [
    "## 2. Setup of the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yTCqemyt8la"
   },
   "source": [
    "Similarly to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3ShseCwt8lb"
   },
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import sklearn naive.bayes classifier library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# import sklearn support vector classifier (svc) library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFnbcu4yt8le"
   },
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLbxWoZit8lf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_mWNdpVUJgJ"
   },
   "source": [
    "Ignore potential library warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhtZlKmUG3_"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsFqwDkYt8ln"
   },
   "source": [
    "Use the `Seaborn` plotting style in all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMH7Y9-Ht8lo"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z2tRqzFt8lu"
   },
   "source": [
    "Set random seed of all our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzE1FzaSt8lu"
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting such a seed insures **reproducibility of the experiments**. In general, the seed function is used to **save the state of a random function**, so that it can generate same random numbers on multiple executions of the code on the same machine or on different machines (for a specific seed value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5MbyOLHVzo5"
   },
   "source": [
    "## 3. Data Download, Assessment and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0gpZzk5t8l5"
   },
   "source": [
    "### 3.1 Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cilrWTyMt8l6"
   },
   "source": [
    "The **Iris Dataset** is a classic and straightforward dataset often used as a \"Hello World\" example in multi-class classification. This data set consists of measurements taken from three different types of iris flowers (referred to as **Classes**),  namely the Iris Setosa, the Iris Versicolour and the Iris Virginica, and their respective measured petal and sepal length (referred to as **Features**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlF-VYuOt8l7"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"iris_dataset.png\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBHv_Rbrt8l8"
   },
   "source": [
    "In total, the dataset consists of **150 samples** (50 samples taken per class) as well as their corresponding **4 different measurements** taken for each sample. Please, find below the list of the individual measurements:\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details of the dataset can be obtained from the following publication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CtBrJGut8l9"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE2PbwClt8mB"
   },
   "source": [
    "Print and inspect the names of the four features contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzLzNDo8t8mF"
   },
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIvnl8Qct8mK"
   },
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq6gZN-1t8mM"
   },
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwiIRMR_t8mW"
   },
   "source": [
    "Determine and print the class label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tayVqRQOt8mX"
   },
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoQlbXs_t8md"
   },
   "source": [
    "Print and inspect the names of the three classes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R__ACqSct8me"
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwqoNt8gt8mh"
   },
   "source": [
    "Let's briefly envision how the feature information of the dataset is collected and presented in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCgJtdiot8mi"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"feature_collection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD3SBLxzt8mi"
   },
   "source": [
    "Let's inspect the **top five** feature rows of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kju1z4Cft8mk"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data, columns=iris.feature_names).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P62AsvZ8t8mr"
   },
   "source": [
    "Let's also inspect the **top five** class labels of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNjr0a5Dt8ms"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target, columns=[\"class\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fxz--vVdt8mu"
   },
   "source": [
    "Let's now conduct a more in depth data assessment. Therefore, we plot the feature distributions of the Iris dataset according to their respective class memberships as well as the features pairwise relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWofkTgQt8mw"
   },
   "source": [
    "Pls. note that we use Python's **Seaborn** library to create such a plot referred to as **Pairplot**. The Seaborn library is a powerful data visualization library based on the Matplotlib. It provides a great interface for drawing informative statstical graphics (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmfO2-yit8mx"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ugPoMiQt8m4"
   },
   "source": [
    "It can be observed from the created Pairplot, that most of the feature measurements that correspond to flower class \"setosa\" exhibit a nice **linear separability** from the feature measurements of the remaining flower classes. In addition, the flower classes \"versicolor\" and \"virginica\" exhibit a commingled and **non-linear separability** across all the measured feature distributions of the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTWFzhhFt8m4"
   },
   "source": [
    "### 3.2 Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTBwny8Dt8m5"
   },
   "source": [
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Please note that the **evaluation set** will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFU5ijYat8m6"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"train_eval_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN25KKcvt8m6"
   },
   "source": [
    "We set the **split fraction** of evaluation data records to **30%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPFvlzS6t8m7"
   },
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FkQME8Ut8m9"
   },
   "source": [
    "Randomly **split the dataset** into (i) a training set and (ii) a evaluation set using the `Scikit-Learn` function `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF7m6KMSt8m9"
   },
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(iris.data, iris.target, test_size=eval_fraction, random_state=random_seed, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T37IuZHIt8m_"
   },
   "source": [
    "Evaluate the **dimensionality** (no. of rows and columns) of the training dataset, denoted as $x^{train}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9i0U2uzt8nA"
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our training dataset **encompasses 105 rows** (approx. 70%) of the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqJitVsit8nC"
   },
   "source": [
    "Evaluate the **dimensionality** (no. of rows and columns) of the evaluation dataset, denoted as $x^{eval}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeVTeCNat8nD"
   },
   "outputs": [],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, our evaluation data **encompasses 45 rows** (approx 30%) of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMSfpCPvt8l4"
   },
   "source": [
    "## 4. Gaussian \"Naive-Bayes\" (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns_yibVst8nK"
   },
   "source": [
    "One popular (and remarkably simple) **supervised machine learning algorithm** is the **Naive Bayes Classifier**. Note, that one natural way to adress a given classification task is via the probabilistic question: **\"What is the most likely class $c^{*}$ considering all the available information $x$?\"** Formally, we wish to output a conditional probability $P(c|x)$ for each class $c$ given distinct observations of $x$. Once we obtained such conditional probability for each class we select the class $c^{*}$ corresponding to the highest $P(c|x)$ as expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q966Q0Hst8nM"
   },
   "source": [
    "$$c^{*} = \\arg \\max_{c} P(c|x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q1VuqAkt8nN"
   },
   "source": [
    "That would require that we need to be prepared to estimate the probability distribution $P(c | \\mathbf{x})$ for every possible value of $\\mathbf{x} = \\{x_1, x_2, ..., x_n\\}$. Here, $P(c | \\mathbf{x})$ denotes the **conditional probability** that is read as  \"the probability of $c$ given $\\mathbf{x}$\". Formally, the conditional probability can be derived from the joint probability and is defined as:\n",
    "\n",
    "$$P(c, \\mathbf{x}) = P(c | \\mathbf{x}) \\cdot P(\\mathbf{x}) \\; \\rightarrow \\; P(c | \\mathbf{x}) = \\frac{P(c, \\mathbf{x})}{P(\\mathbf{x})},$$\n",
    "\n",
    "where $P(c, \\mathbf{x})$ denotes the **joint probability** of $c$ and $\\mathbf{x}$ occurring at the same time.\n",
    "\n",
    "**Excursion:** Imagine a document classification system that, depending on the occurance of a particular set of words in a document, predicts the class of the document. For example, if a the words **\"recipe\"**, **\"pumpkin\"**, **\"cuisine\"**, **\"pancakes\"**, etc. appear in the document, the classifier predicts a high **probability of the document beeing a cookbook**. Let's assume that the feature $x_{pancake} = 1$ might signify that the word **\"pancakes\"** appears in a given document and $x_{pancake} = 0$ would signify that it does not. If we had **30** such binary **\"word-appearence\" features**, that would mean that we need to be prepared to calculate the probability $P(c | \\mathbf{x})$ of any of $2^{30}$ (over 1 billion) possible values of the input vector $\\mathbf{x}= \\{x_1, x_2, ..., x_{30}\\}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auckZh8Dt8nN"
   },
   "source": [
    "$$\\mathbf{x^{1}}= \\{x_1=1, x_2=0, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{2}}= \\{x_1=1, x_2=1, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{3}}= \\{x_1=1, x_2=1, x_3=1, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$...$$\n",
    "$$...$$\n",
    "$$\\mathbf{x^{2^{30}-1}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=0, x_{30}=1\\}$$\n",
    "$$\\mathbf{x^{2^{30}}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=1, x_{30}=1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyrW7n63t8nN"
   },
   "source": [
    "Moreover, where is the learning? If we need to see every single possible example in order to predict the corresponding label then we're not really learning a pattern but just memorizing the dataset. One solution to this challenge is the so-called **Bayes' theorem** (alternatively Bayes' law or Bayes' rule) that you learned about in the lecture. \n",
    "\n",
    "A common scenario for applying the Bayes' theorem formula is when you want to know the probability of something **unobservable** (e.g., the class $c$ of a document) given an **observed** event (e.g., the distinct words $x$ contained in the document). Such a probability is usually referred to as **posterior probability** mathematically denoted by $P(c|x)$.\n",
    "\n",
    "The Bayes' theorem formula provides an elegant way of calculating such posterior probabilities $P(c|x)$ without the need of observing every single possible configuration of $\\mathbf{x} = \\{x_1, x_2, ..., x_n\\}$. Let's briefly revisit the formula of the Bayes' theorem below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8SqN96Kt8nO"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"bayes_theorem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdhvpINqt8nO"
   },
   "source": [
    "In the formula of the **Bayes' theorem** above,\n",
    "\n",
    ">- $P(c|x)$ denotes the **posterior** probability of class $c$ given a set of features $x$ denoted by $x_1, x_2, ..., x_n$.\n",
    ">- $P(c)$ denotes the **prior** probability of observing class $c$.\n",
    ">- $P(x|c)$ denotes the **likelihood** which is the probability of a feature $x$ given class $c$.\n",
    ">- $P(x)$ denotes the **evidence** which is the general probability of observing a feature or feature vector $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDxV-LR9t8nP"
   },
   "source": [
    "### 4.1 Calculation of the prior probabilities $P(c)$ of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-1fKmoht8nP"
   },
   "source": [
    "Let's start building an intuition of the Bayes' theorem by first calculating the prior probability $P(c)$ of each class iris flower contained in the dataset. Therefore, we first obtain the **number of occurrence of each class** in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LuZLk1zt8nQ"
   },
   "outputs": [],
   "source": [
    "# determine counts of unique class labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# concatenate counts and class labels in a python dictionary\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# print obtained dictionary\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onQSHIPyt8nU"
   },
   "source": [
    "Let's now **convert the obtained counts into probabilities**. Therefore, we divide the class counts by the overall number of observations contained in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "godgFvq6t8nV"
   },
   "outputs": [],
   "source": [
    "# divide counts by the number of observations available in the training data\n",
    "prior_probabilities = counts / np.sum(counts)\n",
    "\n",
    "# print obtained probabilites\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bACVm7Rt8na"
   },
   "source": [
    "Let's plot the obtained prior probabilites $P(c)$ accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oiq9nyvRt8na"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.bar(x=np.unique(iris.target), height=prior_probabilities, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$c_{i}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(c_{i})$\", fontsize=14)\n",
    "\n",
    "# set x-axis ticks\n",
    "ax.set_xticks(np.unique(iris.target))\n",
    "\n",
    "# set y-axis range\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the prior class probabilites $P(c)$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isM3cVjHt8ne"
   },
   "source": [
    "### 4.2 Calculation of the evidence $P(x)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyCLzVaft8nf"
   },
   "source": [
    "Let's now calculate the general probability of **observing a particular observation** (or feature vector) $𝑥$. From A Bayes' theorem perspective this denotes the **evidence $P(\\mathbf{x})$** of an observation $x=\\{x_1, x_2, ..., x_n\\}$. We assume that the first feature $x_{1}$ represents the \"sepal length\" observations of the Iris Dataset, the second feature $x_{2}$ = \"sepal width\", $x_{3}$ = \"petal length\", and $x_{4}$ = \"petal width\". In order to calculate the evidence $P(x)$ of a particular observation, e.g, $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$ the **Gaussian Naive Bayes' theorem** in general utilizes the following two tricks:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"** \n",
    "\n",
    "Using the **\"Chain Rule of Probabilities\"**, we can express the evidence term $P( \\mathbf{x} )$ as:\n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2 | x_1) \\cdot P(x_3 | x_1, x_2) \\cdot P(x_4 | x_1, x_2, x_3) \\cdot ... \\cdot P( x_n | x_1, ..., x_{n-1}) = \\prod^n_i P(x_i | x_{1:i-1})$$\n",
    "\n",
    "By itself, this expression doesn't get us any further. We still need, even in a case of $d$ binary features, to estimate roughly $2^d$ parameters. The trick of the **naive** Bayes theorem however is to **assume that the distinct features $x_1, x_2, ..., x_n$ are conditionally independent of each other** when observing a particular class $c$. Using this assumption we're in much better shape, as the evidence term $P(\\mathbf{x})$ simplifies to: \n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2) \\cdot P(x_3) \\cdot P(x_4) \\cdot ... \\cdot P( x_n ) = \\prod^n_i P(x_i)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i)$ amounts to estimating the distribution of each feature $x_i$ independently. As a result, the assumption of conditional independence reduced the complexity of our model (in terms of the number of parameters) from an **exponentially growing dependence** in the number of features to a **linear growing dependence**. Hence, we call it the **\"naive\"** Bayes' theorem, since it makes the naive assumption about feature independence, so we don't have to care about dependencies among them.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"** \n",
    "\n",
    "During the lecture you learned that the **evidence distribution can be approximated by a Gaussian (Normal) probability distribution** $\\mathcal{N}(\\mu, \\sigma)$. This simplification can be justified by the application of the **\"Law of Large Numbers\"** or **\"Central Limit Theorem\"** (you may want to have a look at further details of the theorem under: https://en.wikipedia.org/wiki/Central_limit_theorem). In general, the probability density of a Gaussian \"Normal\" distribution, as defined by the formula below, is parametrized its **mean $\\mu$** and corresponding **standard deviation $\\sigma$**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0D2oVwht8ng"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 550px; height: auto\" src=\"evidence_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZNZwtLmt8ng"
   },
   "source": [
    "Using the **\"Law of Large Numbers\"** we will approximate the evidence probability density $P(x) \\approx \\mathcal{N}(x | \\mu, \\sigma)$ of each of each feature $x_i$ by a Gaussian. To achieve this we need to come up with a good estimate of the parameters $\\mu$ and $\\sigma$ that define a Gaussian (Normal) probability distribution.\n",
    "\n",
    "But how can this be achieved in practice? Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the Iris Dataset. The following line of code determines a histogram of the true **sepal length** feature value distribution and plots it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxpYEpTMt8ng"
   },
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution\n",
    "hist_probabilities, hist_edges = np.histogram(x_train[:, 0], bins=10, range=(0,10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_probabilities)\n",
    "\n",
    "# print the histogram edges\n",
    "print(hist_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yKoA2H7t8nm"
   },
   "source": [
    "Let's also plot the probability density of the **sepal length feature $x_{1}$** accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yuqovydut8nn"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1})$\", fontsize=14)\n",
    "\n",
    "# set the y-axis limitations\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH9Lh02tt8nq"
   },
   "source": [
    "How can we approximate the true probability density of the **sepal length** feature using a Gaussian distribution? Well, all we need to do is to calculate it's mean $\\mu_{1}$ and standard deviation $\\sigma_{1}$. Let's start by calculating the mean $\\mu_{1}$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9qFq35at8nq"
   },
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations\n",
    "mean_sepal_length = np.mean(x_train[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOAWwXSJt8nu"
   },
   "source": [
    "Let's continue by calculating the standard devition $\\sigma_{1}$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4JZh0Vtt8nv"
   },
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations\n",
    "std_sepal_length = np.std(x_train[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "votKuOxDt8ny"
   },
   "source": [
    "We can now determine the approximate Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu_{1}, \\sigma_{1})$ of the **sepal length** feature using $\\mu_{1}$ and $\\sigma_{1}$ obtained above. Thereby, we will utilize the `pdf.norm` function available in the `scipy.stats` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-BMdABnt8n1"
   },
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length, std_sepal_length)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs9C5T06t8n4"
   },
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x_{1}}) \\approx \\mathcal{N}(\\mu_{1}, \\sigma_{1})$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xai5fFIft8n5"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}=$sepal-length\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1}=$sepal-length$)$\", fontsize=14)\n",
    "\n",
    "# set the y-axis limitations\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "817uNi_8t8n8"
   },
   "source": [
    "Let's likewise approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x_{2}}) \\approx \\mathcal{N}(\\mu_{2}, \\sigma_{2})$ of the **sepal width** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO6QUCWSt8n9"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature\n",
    "mean_sepal_width = np.mean(x_train[:, 1])\n",
    "std_sepal_width = np.std(x_train[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width, std_sepal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal width\" observations\n",
    "ax.hist(x_train[:, 1], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{2}=$sepal-width\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{2}=$sepal-width$)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4C79lskt8oB"
   },
   "source": [
    "And approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x_{3}}) \\approx \\mathcal{N}(\\mu_{3}, \\sigma_{3})$ of the **petal length** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrfIjMgSt8oB"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature\n",
    "mean_petal_length = np.mean(x_train[:, 2])\n",
    "std_petal_length = np.std(x_train[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length, std_petal_length), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal length\" observations\n",
    "ax.hist(x_train[:, 2], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{3}=$petal-length\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{3}=$petal-length$)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubh_eyqzt8oG"
   },
   "source": [
    "And approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x_{4}}) \\approx \\mathcal{N}(\\mu_{4}, \\sigma_{4})$ of the **petal width** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-JArHg7t8oI"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature\n",
    "mean_petal_width = np.mean(x_train[:, 3])\n",
    "std_petal_width = np.std(x_train[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width, std_petal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal width\" observations\n",
    "ax.hist(x_train[:, 3], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{4}=$petal-width\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{4}=$petal-width$)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxElzxqt8oM"
   },
   "source": [
    "### 4.3 Calculation of the likelihood $P(x|c)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0GZ9F4t8oN"
   },
   "source": [
    "Let's now see how we can calculate the **likelihood** $P(\\mathbf{x}|c)$ which is the probability density of a feature given a particular class $c$. We will again make use of the two tricks that we applied when calculating the **evidence** $P(x)$ probabilities. In order to calculate the likelihood $P(x|c)$ of a particular observation, e.g, $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5 | c=\"setosa\"\\}$ we will apply:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"**, using the **\"Chain Rule of Probabilities\"**, we can express the likelihood term $P( \\mathbf{x} | c)$ as:\n",
    "\n",
    "$$P( \\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1, c) \\cdot P(x_2 | x_1, c) \\cdot P(x_3 | x_1, x_2, c) \\cdot P(x_4 | x_1, x_2, x_3, c) \\cdot ... \\cdot = \\prod^n_i P(x_i | x_{1:i-1}, c)$$\n",
    "\n",
    "We will again assume that the distinct features $x_1, x_2, ..., x_n$ are conditionally independent from each other when observing a particular class $c$. As a result the likelihood term $P( \\mathbf{x} | c)$ simplifies to: \n",
    "\n",
    "$$P( \\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1 | c) \\cdot P(x_2 | c) \\cdot P(x_3 | c) \\cdot P(x_4 | c) \\cdot ... \\cdot P( x_n | c) = \\prod^n_i P(x_i | c)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i | c)$ amounts to estimating the distribution of each feature $x_i$ independently.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"**, using this simplification we can can estimate $P(\\mathbf{x}|c)$ by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. The **likelihood** probability density of a Gaussian \"Normal\" distribution, as defined by the formula below, is determined by its mean $\\mu$, standard deviation $\\sigma$ and it's corresponding class condition $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvDGdWgct8oO"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 550px; height: auto\" src=\"likelihood_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jhj7TARt8oP"
   },
   "source": [
    "Using the **\"Law of Large Numbers\"** we will approximate the likelihood probability density $P(x | c) \\approx \\mathcal{N}(x | \\mu, \\sigma, c)$ of each of each feature $x_i$ by a Gaussian. To achieve this we need to come up with a good estimate of the parameters $\\mu$ and $\\sigma$ that define a Gaussian (Normal) probability distribution.\n",
    "\n",
    "But how can this be achieved in practice? Let's start by applying the class conditioning. This is usually done by filtering the dataset for each class $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qOYvwmBt8oQ"
   },
   "outputs": [],
   "source": [
    "# collect all iris setosa measurements, class label = 0\n",
    "x_train_setosa = x_train[y_train == 0]\n",
    "\n",
    "# collect all iris versicolor measurements, class label = 1\n",
    "x_train_versicolor = x_train[y_train == 1]\n",
    "\n",
    "# collect all iris virginica measurements, class label = 2\n",
    "x_train_virginica = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlUNbkBct8ob"
   },
   "source": [
    "Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the iris dataset given the class **setosa**. The following line of code determines a histogram of the true feature value distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBK78B_Ht8oc"
   },
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution given the class \"setosa\"\n",
    "hist_setosa, bin_edges_setosa = np.histogram(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_setosa)\n",
    "\n",
    "# print the histogram edges\n",
    "print(bin_edges_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71hyDanft8oj"
   },
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh3aL0zft8oj"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$=sepal-length\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1}$=sepal-length$|c_{1}=$setosa)\", fontsize=14)\n",
    "\n",
    "# set y-axis limitations\n",
    "ax.set_ylim([0.0, 1.2])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature given class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXBXHssEt8ol"
   },
   "source": [
    "We are again able to determine the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}_{1} | c_{1}=setosa) \\approx \\mathcal{N}(\\mu_{5}, \\sigma_{5} | c_{1}=setosa)$ of the **sepal length** feature given the class **setosa** using again the `pdf.norm` function of the `scipy.stats` package.\n",
    "\n",
    "Let's continue by calculating the mean $\\mu_{5}$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOw03Lt7t8ol"
   },
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations given class \"setosa\"\n",
    "mean_sepal_length_setosa = np.mean(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEtJIBt7t8on"
   },
   "source": [
    "Let's continue by calculating the standard devition $\\sigma_{5}$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0IQ6olCt8oq"
   },
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations given class \"setosa\"\n",
    "std_sepal_length_setosa = np.std(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN4Vs2I6t8or"
   },
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length_setosa = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length_setosa, std_sepal_length_setosa)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URVaONMet8ou"
   },
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}_{1} | c_{1}=setosa) \\approx \\mathcal{N}(\\mu_{5}, \\sigma_{5} | c_{1}=setosa)$ of the **sepal length** feature given class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWk9d7Xit8ow"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length_setosa, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$=sepal-length\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1}$=sepal-length$|c_{1}=$setosa)\", fontsize=14)\n",
    "\n",
    "# set y-axis limitations\n",
    "ax.set_ylim([0.0, 1.2])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature given class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8r3rTe2t8oy"
   },
   "source": [
    "Let's likewise approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}_{2} | c_{1}=setosa) \\approx \\mathcal{N}(\\mu_{6}, \\sigma_{6} | c_{1}=setosa)$ of the **sepal width** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGFCoXIDt8oz"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature given class setosa\n",
    "mean_sepal_width_setosa = np.mean(x_train_setosa[:, 1])\n",
    "std_sepal_width_setosa = np.std(x_train_setosa[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width_setosa, std_sepal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 1], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{2}$=sepal-width\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{2}$=sepal-width$|c_{1}=$setosa)\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" feature given class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tBsXdGit8o1"
   },
   "source": [
    "And approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}_{3} | c_{1}=setosa) \\approx \\mathcal{N}(\\mu_{7}, \\sigma_{7} | c_{1}=setosa)$ of the **petal length** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8Jp6pJyt8o1"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature given class setosa\n",
    "mean_petal_length_setosa = np.mean(x_train_setosa[:, 2])\n",
    "std_petal_length_setosa = np.std(x_train_setosa[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length_setosa, std_petal_length_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 2], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{3}$=petal-length\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{3}$=petal-length$|c_{1}=$setosa)\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" feature given class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq-1aIsUt8o3"
   },
   "source": [
    "And approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}_{4} | c_{1}=setosa) \\approx \\mathcal{N}(\\mu_{8}, \\sigma_{8} | c_{1}=setosa)$ of the **petal width** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNQBndxEt8o3"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature given class setosa\n",
    "mean_petal_width_setosa = np.mean(x_train_setosa[:, 3])\n",
    "std_petal_width_setosa = np.std(x_train_setosa[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width_setosa, std_petal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 3], bins=10, range=(0, 10), density=True, color='green')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{4}$=petal-width\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{4}$=petal-width$|c_{1}=$setosa)\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" feature given class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9MU5rOtN4Ol"
   },
   "source": [
    "Compute mean and standard deviations of the **'versicolor'** class distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlKsJtpFN4Ol"
   },
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'versicolor'\n",
    "mean_sepal_length_versicolor = np.mean(x_train_versicolor[:, 0])\n",
    "std_sepal_length_versicolor = np.std(x_train_versicolor[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'versicolor'\n",
    "mean_sepal_width_versicolor = np.mean(x_train_versicolor[:, 1])\n",
    "std_sepal_width_versicolor = np.std(x_train_versicolor[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'versicolor'\n",
    "mean_petal_length_versicolor = np.mean(x_train_versicolor[:, 2])\n",
    "std_petal_length_versicolor = np.std(x_train_versicolor[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'versicolor'\n",
    "mean_petal_width_versicolor = np.mean(x_train_versicolor[:, 3])\n",
    "std_petal_width_versicolor = np.std(x_train_versicolor[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmMoqdM_N4Ol"
   },
   "source": [
    "Compute mean and standard deviations of the **'virginica'** class distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az8I7gtmN4Ol"
   },
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'virginica'\n",
    "mean_sepal_length_virginica = np.mean(x_train_virginica[:, 0])\n",
    "std_sepal_length_virginica = np.std(x_train_virginica[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'virginica'\n",
    "mean_sepal_width_virginica = np.mean(x_train_virginica[:, 1])\n",
    "std_sepal_width_virginica = np.std(x_train_virginica[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'virginica'\n",
    "mean_petal_length_virginica = np.mean(x_train_virginica[:, 2])\n",
    "std_petal_length_virginica = np.std(x_train_virginica[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'virginica'\n",
    "mean_petal_width_virginica = np.mean(x_train_virginica[:, 3])\n",
    "std_petal_width_virginica = np.std(x_train_virginica[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu7zW41rt8o_"
   },
   "source": [
    "### 4.4 Calculation of the posterior probability $P(c|x)$ of unknown iris flower observations $x^{s}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbfZXyolt8o_"
   },
   "source": [
    "Now we have determined all the distinct elements $P(c)$, $P(x)$ and $P(x|c)$ of the Bayes' theorem the determine the posterior probability $P(c_{1}=setosa|x)$ of a so far unseen \"new\" observations x of class **setosa**. Let's therefore determine if two so far unseen **iris flower** observations correspond to class **setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Q3j13Jt8pA"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"iris_sample_1.png\">\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQVe6u3Nt8pB"
   },
   "source": [
    "The first **iris flower** observation $x^{s1}$ exhibits the following observed feature values: $x^{s1} = \\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo6DTk2Et8pC"
   },
   "outputs": [],
   "source": [
    "# init features of first iris flower observation \n",
    "sepal_length = 5.8 \n",
    "sepal_width  = 3.5\n",
    "petal_length = 1.5\n",
    "petal_width  = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjGgwq6Jt8pE"
   },
   "source": [
    "Let's build an intuition of the distinct iris flower class distributions including the current iris flower observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQPyxQNZt8pH"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observation to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([[5.8, 3.5, 1.5, 0.25, \"observation 1\"]], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bikfmTDIt8pM"
   },
   "source": [
    "Let's determine the posterior probability $P(c_{1}=setosa|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDPPyce5t8pN"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0]\n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x)\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oovIgl5yt8pR"
   },
   "source": [
    "Ok, our observed iris flower results in a **posterior probability** $P(c_{1}=setosa|x^{s1})$ **of beeing of class setosa of approx. 99%**. For comparison purposes, let's also determine the posterior probability $P(c_{2}=versicolor|x^{s1})$ by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAqY-tFkt8pT"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCdb5B8vt8pW"
   },
   "source": [
    "As well as the posterior probability $P(c_{3}=virginica|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkYN1Ax3t8pX"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRY5tGXut8pb"
   },
   "source": [
    "Based on the obtained posterior probabilites $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$:\n",
    "\n",
    "$$P(c_{1}=setosa \\;| \\;x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{0.9999} \\approx 99.99\\% $$\n",
    "$$P(c_{2}=versicolor \\; | \\;x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{4.69e^{-14}} \\approx 0\\%$$\n",
    "$$P(c_{3}=virginica\\; |\\;x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{2.20e^{-21}} \\approx 0\\%$$\n",
    "\n",
    "we can now apply the **initial classification criteria, denoted by $\\arg \\max_{c} P(c|x)$** to safely determine the **observation's most likely class $c^{*} = setosa$**.\n",
    "\n",
    "Let's now have a look at a second **iris flower** observation and determine its most likely class $c^{*}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fjYMMiPt8pc"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"iris_sample_2.png\">\n",
    "\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKat5TXkt8pc"
   },
   "source": [
    "The second **iris flower** observation $x^{s2}$ exhibits the following observed feature values: $x^{s2} = \\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiqXvhkJt8pd"
   },
   "outputs": [],
   "source": [
    "# init a second random feature observation \n",
    "sepal_length = 7.8\n",
    "sepal_width  = 2.3\n",
    "petal_length = 6.4\n",
    "petal_width  = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbrVL1-2t8pj"
   },
   "source": [
    "Let's again build an intuition of the distinct iris flower class distributions including the current iris flower observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyFkHCKKt8pk"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observations to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([[7.8, 2.3, 6.4, 2.50, \"observation 2\"]], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-S8RLkst8pn"
   },
   "source": [
    "Let's determine the posterior probability $P(c=setosa|x^{s2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jr_BxeZt8pn"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0] \n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x)\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wevGXIlYt8pu"
   },
   "source": [
    "Ok, our observed iris flower results in a **very low posterior probability** $P(c_{1}=setosa|x^{s2})$ **of beeing of class setosa of approx. 0%**. For comparison purposes, let's also determine the posterior probability $P(c_{2}=versicolor|x^{s2})$ and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOcWllFpt8pu"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cPjox7Nt8py"
   },
   "source": [
    "As well as the posterior probability $P(c_{3}=virginica|x^{s2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12-0tBpDt8pz"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI6MpHGAt8p1"
   },
   "source": [
    "Based on the obtained posterior probabilites $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:\n",
    "\n",
    "$$P(c_{1}=setosa \\; |\\; x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.24e^{-268}} \\approx 0\\%$$\n",
    "$$P(c_{2}=versicolor \\; | \\; x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.12e^{-12}} \\approx 0\\%$$\n",
    "$$P(c_{3}=virginica \\; | \\; x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{0.9999} \\approx 99.99\\%$$\n",
    "\n",
    "we can again apply the **initial classification criteria, denoted by** $\\arg \\max_{c} P(c|x)$ **to savely determine the observations most likely class** $c^{*} = virginica$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYPrkS8Qt8p1"
   },
   "source": [
    "### 4.5 Training and utilization of a Gaussian Naive-Bayes Classifier using Python's Sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wdS9rZvt8p1"
   },
   "source": [
    "Luckily, there is a Python library named `Scikit-Learn` (https://scikit-learn.org) that provides a variety of machine learning algorithms that can be easily interfaced using the Python programming language. It also contains supervised classification algorithms such as the **Gaussian Naive-Bayes** classifier which we can use of the shelf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-475VY9t8p1"
   },
   "source": [
    "Let's use `Scikit-Learn` and instantiate the **Gaussian Naive-Bayes** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CKOrUEit8p2"
   },
   "outputs": [],
   "source": [
    "# init the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB(priors=None, var_smoothing=1e-09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omozNYbKt8p4"
   },
   "source": [
    "**Train or fit the Gaussian Naive-Bayes classifier** using the training dataset features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPaxZNUzt8p4"
   },
   "outputs": [],
   "source": [
    "# train the Gaussian Naive Bayes classifier\n",
    "gnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTPCVjM1t8p7"
   },
   "source": [
    "**Utilize the trained Gaussian Naive Bayes model** to predict the classes of the distinct observations contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8hZtWyDt8p9"
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZisFYfjt8p_"
   },
   "source": [
    "Let's have a look at the class labels **predicted** by the Gaussian Naive-Bayes model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aWJ9PGkt8qA"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GldhBv1gt8qD"
   },
   "source": [
    "As well as the **true** class labels as contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hUU5UDkt8qD"
   },
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXcfnX7ut8qH"
   },
   "source": [
    "Determine the **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUjNPSw4t8qH"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAR7qFaht8qJ"
   },
   "source": [
    "Determine number of **missclassified** data sampels in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7s8UEK8Vt8qJ"
   },
   "outputs": [],
   "source": [
    "print(\"Number of mislabeled points out of a total {} points: {}\".format(x_eval.shape[0], np.sum(y_eval != y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_xAgsV6t8qL"
   },
   "source": [
    "In the field of machine learning and in particular the field of statistical classification, a **confusion matrix**, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the number of instances that the classifier predicted per class, while each column represents the instances of the true or actual class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv_p7Z_3t8qL"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 300px; height: auto\" src=\"confusion_matrix.png\">\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jRIduF8t8qM"
   },
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjLxhnrOt8qO"
   },
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAtUqq_vt8qR"
   },
   "source": [
    "Plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-_WFNpVt8qS"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]')\n",
    "plt.ylabel('[predicted label]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('Gaussian Naive Bayes Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZtOsS5ft8qY"
   },
   "source": [
    "Let's now use the learned model and apply it to our **unknown observations $x^{s1}$ and $x^{s2}$** to determine the **corresponding class predictions $c^{*}$** of the model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3kPaqtKt8qY"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the first unknown observation\n",
    "class_prediction_sample_1 = gnb.predict([[5.8, 3.5, 1.5, 0.25]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_1[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI0efIyst8qb"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the second unknown observation\n",
    "class_prediction_sample_2 = gnb.predict([[7.8, 2.3, 6.4, 2.50]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_6MsT_JYZLu"
   },
   "source": [
    "## 5. Support Vector Machine (SVM) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now switch gears and try to solve the Iris Dataset classification challenge using a discriminative classifier. We will opt for the **\"Support Vector Machine (SVM) Classifier\"** as introduced in the lecture. Before doing so, let's briefly visit the general idea of **discriminative classifiers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px; height: auto\" src=\"classifiers.png\">\n",
    "\n",
    "(Courtesy: Prof. Dr. Borth, University of St. Gallen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUSvKz39bVj4"
   },
   "source": [
    "Let's suppose we are given $l$ observations. Each observation consists of a pair: a vector $x_{i} \\in \\mathbb{R}^{n}, i=1, ..., l$ and the associated \"truth\" $y_{i}$, provided by a trusted source. In the context of a face detection task, $x_{i}$ might be vector of pixel values (e.g. $n$=256 for 1024x1024 pixel image), and $y_{i}$ would be $1$ if the image contains a face, and $-1$ otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z935bhnQbqNC"
   },
   "source": [
    "### 5.1 Linear Support Vector Machine (SVM) Classifiers - The Linear Separable Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_dBzv_cbwZg"
   },
   "source": [
    "Suppose we have some hyperplane which separates the positive from the negative examples referred to as \"separating hyperplane\". The points $x$ which lie on the hyperplane satisfy the following equation $w \\cdot x + b = 0$, where $w$ is normal to the hyperplane, $|b|/||w||$ is the perpendicular distance from the hyperplane to the origin, and $||w||$ is the Euclidean norm of $w$. Let $d_{+}$ ($d_{-}$) be the shortest distance from the separating hyperplane to the closest positive (negative) example. We define the \"margin\" of a separating hyperplane to be $d_{+} + d_{-}$. In the context of the linearly separable case, the support vector algorithm simply looks for the separating hyperplane with the maximum margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6tz_pk1b3p1"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"hyperplane_linear.png\">\n",
    "\n",
    "Linear separating hyperplanes $H_{1}$, $H_{2}$, and $H^{*}$ for the separable case. The support vectors that constitute $H_{1}$, $H_{2}$ are circled.\n",
    "\n",
    "(Source: https://link.springer.com/article/10.1023/A:1009715923555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oFJGGfMb-Xo"
   },
   "source": [
    "Suppose that **all the training data** satisfies the following constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RYdT5UIcAlM"
   },
   "source": [
    "$$ x_{i} \\cdot w + b \\geq + 1, y_{i} = +1 $$\n",
    "\n",
    "$$ x_{i} \\cdot w + b \\leq - 1, y_{i} = -1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLvwtnYVcC97"
   },
   "source": [
    "This can be combined into **one set of inequalities**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJxB3kEXcFJZ"
   },
   "source": [
    "$$y_{i}(x_{i} \\cdot w + b) - 1 \\geq 0, \\forall_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQmlmrF3cIuS"
   },
   "source": [
    "Let's now consider the points for which the equality $x_{i} \\cdot w + b \\geq + 1$ holds. These points lie on a hyperplane $H_{1}: x_{i} \\cdot w + b = + 1$ with normal $w$ and perpendicular distance from the origin $|1-b|/||w||$. Similarly, the points for which the equality $x_{i} \\cdot w + b \\leq - 1$ holds lie on the hyperplane $H_{2}: x_{i} \\cdot w + b = -1$, with normal again $w$, and perpendicular distance from the origin $|-1-b|/||w||$. Hence $d_{+} = d_{-} = 1 / ||w||$ and the margin is simply 2/||w||. Note that $H_{1}$ and $H_{2}$ are parallel and that no training points $x_{i}$ fall between them. Thus we can find a pair of hyperplanes which correspond to a maximum margin by minimizing $||w||^{2}$, subject to constraint $y_{i}(x_{i} \\cdot w + b) - 1 \\geq 0$. Those training points $x_{i}$ which wind up lying on one of the hyperplanes $H_{1}$, $H_{2}$, and whose removal would change the solution found, are referred to as **\"support vectors\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-NtlQxyFkid"
   },
   "source": [
    "#### 5.1.1 A \"Primal\"  Optimization Objective Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf7nIsMHFkid"
   },
   "source": [
    "As discussed in the lecture, we can reformulate the objective of finding such a max-margin seperating hyperplane as a Lagrangian optimization objective. Thereby, we introduce a set of positive Lagrange multipliers $\\alpha_{i}, i=1, ..., l$ which turns the search for a max-margin seperating hyperplane into solving the following Lagrangian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzl08-LFkid"
   },
   "source": [
    "$$\\underset{w, \\alpha}{\\arg \\min} \\; L_{P} = \\frac{1}{2}||w||^{2} - \\sum_{i=1}^{l} \\alpha_{i}y_{i}(x_{i} \\cdot w + b) + \\sum_{i=1}^{l}\\alpha_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1EkyuBXFkid"
   },
   "source": [
    "We must now minimize $L_{P}$, referred to as the **\"primal\"**, with respect to $w$, $b$. Thereby, \n",
    "\n",
    "> 1. the minimization of the first term $\\frac{1}{2}||w||^{2}$ maximizes the margin of the separating hyperplane, \n",
    "> 2. the maximization of the second term $\\sum_{i=1}^{l} \\alpha_{i}y_{i}(x_{i} \\cdot w + b)$ maximizes the number of correctly classfied training samples,\n",
    "> 3. the minimization of the third term $\\sum_{i=1}^{l}\\alpha_{i}$ minimizes the number of support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1lEimKdFkid"
   },
   "source": [
    "Minimization of $L_{P}$ is a **convex quadratic programming problem**, since the objective function is itself convex, and those points for which $\\alpha_{i} > 0$ that satisfy the constraints also form a convex set. Again, those points are called **\"support vectors\"**, and lie on one of the hyperplanes $H_{1}$, $H_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W31_jb6SFkie"
   },
   "source": [
    "#### 5.1.2 A \"Dual\" Optimization Objective Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJme8aUwFkie"
   },
   "source": [
    "Requiring that the gradient of $L_{P}$ with respect to $w$ and $b$ vanish result in the conditions, that $w = \\sum_{i=1}^{l} \\alpha_{i}y_{i}x_{i}$ and $\\sum_{i=1}^{l}\\alpha_{i}y_{i} = 0$. Using those conditions, the above shown Lagrangian can be reformulated to derive its **\"dual\"** formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy7T0pHyFkie"
   },
   "source": [
    "$$\\underset{\\alpha}{\\arg \\min} \\; L_{D} = \\sum_{i=1}^{l}\\alpha_{i} + \\frac{1}{2} \\sum_{i,j=1}^{l} \\alpha_{i}\\alpha_{j}y_{i}y_{j}<x_{i}, x_{j}>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTR66zrZFkif"
   },
   "source": [
    "Note that solving the dual formulation doesn't depend on $w$ anymore. It only depends on the samples $x_{i} \\in \\mathbb{R}^{n}, i=1, ..., l$ of the training dataset as well as the associated labels $y_{i}$. This indicates that the optimal seperating hyperplane $H^{*}$ becomes a linear function of the data. Note also that if we formulate the problem, as above, with $b=0$, requires that all hyperplanes contain the origin. However, this is a mild restriction for high dimensional spaces since it amounts to reducing the number of degrees of freedom by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfm7EQIMFkif"
   },
   "source": [
    "#### 5.1.3 Training of a Linear Support Vector Machine (SVM) Classifer using Python's Scikit-Learn Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhayL3XcFkif"
   },
   "source": [
    "Luckily, the `Scikit-Learn` (https://scikit-learn.org) machine learning library provides a variety of machine learning algorithms that can be easily interfaced using the Python programming language. Among others the library also contains a variety of supervised classification algorithms such as the **Support Vector Machine (SVM)** classifier. The SVM classifier can be trained \"off-the-shelf\" to solve the dual Lagrangian $L_{D}$ optimization objective formulated above. Let's instantiate one of the SVM classifiers available in `Scikit-Learn` to learn a linear seperating hyperplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNkc8l9gFkif"
   },
   "outputs": [],
   "source": [
    "# init the Support Vector Machine classifier\n",
    "svm = SVC(kernel='linear', random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svdz0J_1Fkif"
   },
   "source": [
    "**Train or fit the SVM classifier** using the training dataset features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KtvHUPyFkif"
   },
   "outputs": [],
   "source": [
    "# train / fit the Support Vector Machine classifier\n",
    "svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waanZHZhFkig"
   },
   "source": [
    "#### 5.1.4 Evaluation of the trained Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpA2MyxlFkig"
   },
   "source": [
    "After fitting the training data, the optimal seperating hyperplane $H^{*}$ learned by the SVM model can then be used to predict the corresponding class labels $y_{i}'$ of so far unknown observations $x_{i}'$. We will utilize the trained model to predict the class labels of the remaining observations contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBDb-hDtFkig"
   },
   "outputs": [],
   "source": [
    "y_pred = svm.predict(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNZ9AY42Fkig"
   },
   "source": [
    "Let's have a look at the class labels $y_{i}'$ **predicted** by the SVM classifier on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czDo3PN9Fkig"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeF0Ng0ZFkig"
   },
   "source": [
    "As well as the **true** class labels $y_{i}$ as contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hyd7KYRdFkih"
   },
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQnl3YaiFkih"
   },
   "source": [
    "Ok, comparing the **true** and **predicted** class labels looks encouraging. Let's determine the exact **prediction accuracy** that the trained model $h$ was able to achieve on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zfMMPF3Fkih"
   },
   "outputs": [],
   "source": [
    "print('Model classification accuracy: {}%'.format(str(metrics.accuracy_score(y_eval, y_pred) * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mt4cWuTFkih"
   },
   "source": [
    "Determine the number of **misclassified** data sampels in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RibujFnFkih"
   },
   "outputs": [],
   "source": [
    "print('Number of mislabeled points out of a total {} points: {}'.format(x_eval.shape[0], np.sum(y_eval != y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nd8iFDBFkih"
   },
   "source": [
    "In the field of machine learning and in particular the field of statistical classification, a **confusion matrix**, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the number of instances that the classifier predicted per class, while each column represents the instances of the true or actual class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3hm-IBFFkii"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 300px; height: auto\" src=\"confusion_matrix.png\">\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLpCk1sYFkii"
   },
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgL9R77dFkii"
   },
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF3FTVPTFkii"
   },
   "source": [
    "Plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkDuPzYRFkii"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='BuGn_r', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true class label $y_{i}$]')\n",
    "plt.ylabel('[predicted class label $y_{i}\\'$]')\n",
    "\n",
    "# add plot title\n",
    "plt.title('SVM Predictions - Confusion Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-vbaAiUFkii"
   },
   "source": [
    "#### 5.1.5 Prediction of Classes of Unknown Iris Flower Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U4Lvk6iFkij"
   },
   "source": [
    "**First unknown iris flower:** Now that we have trained and evaluated our SVM classifier let's apply it to two so far unknown or unseen **iris flower** observations. The first **iris flower** observation $x^{s1}$ exhibits the following observed feature values: $x^{s1} = \\{x_{sl}=5.8, x_{sw}=3.5, x_{pl}=1.5, x_{pw}=0.25\\}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQI5S4bWFkij"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 200px; height: auto\" src=\"iris_sample_1.png\">\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QonoOGvjFkij"
   },
   "source": [
    "Let's convert those measurements into a feature vector $x^{s1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kH6gO9jFkij"
   },
   "outputs": [],
   "source": [
    "# init features of the first unknown iris flower observation \n",
    "sepal_length = 5.8 \n",
    "sepal_width  = 3.5\n",
    "petal_length = 1.5\n",
    "petal_width  = 0.25\n",
    "\n",
    "# create the observation feature vector\n",
    "x_s1_feature_vector = [sepal_length, sepal_width, petal_length, petal_width]\n",
    "\n",
    "# print the feature vector\n",
    "print(x_s1_feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URoP5fpXFkij"
   },
   "source": [
    "Let's now use our trained SVM model $h$ to predict the class $c^{*}$ of the unknown iris flower $x^{s1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r05CTt92Fkij"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the first unknown observation\n",
    "class_prediction_sample_1 = svm.predict([x_s1_feature_vector])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_1[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZuZTnYFkij"
   },
   "source": [
    "Let's build an intuition of the distinct iris flower class distributions including the current iris flower observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2gr-TE4Fkik"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset('iris')\n",
    "\n",
    "# add preliminary label to unknown feature observation\n",
    "x_s1_feature_vector.append('observation s1')\n",
    "\n",
    "# add observation to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([x_s1_feature_vector], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIUtql-9Fkik"
   },
   "source": [
    "Ok, the feature distributions of the feature values observable for the unknown iris flower $x^{s1}$ exhibit a high likelihood of beeing of class **setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DwkMMRdFkik"
   },
   "source": [
    "**Second unknown iris flower:** Let's apply the learned SVM model to a second unknown or unseen **iris flower** observations. The second **iris flower** observation $x^{s2}$ exhibits the following observed feature values $x^{s2} = \\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWb47pQvFkik"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 200px; height: auto\" src=\"iris_sample_2.png\">\n",
    "\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XGMtmuNFkik"
   },
   "source": [
    "Let's again convert those measurements into a feature vector $x^{s2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2ShU5YKFkik"
   },
   "outputs": [],
   "source": [
    "# init features of the second unknown iris flower observation \n",
    "sepal_length = 7.8\n",
    "sepal_width  = 2.3\n",
    "petal_length = 6.4\n",
    "petal_width  = 2.5\n",
    "\n",
    "# create the observation feature vector\n",
    "x_s2_feature_vector = [sepal_length, sepal_width, petal_length, petal_width]\n",
    "\n",
    "# print the feature vector\n",
    "print(x_s2_feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cr696IMFkil"
   },
   "source": [
    "Use the trained SVM model $h$ to predict the class $c^{*}$ of the unknown iris flower $x^{s2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZccmoxkwFkil"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the first unknown observation\n",
    "class_prediction_sample_2 = svm.predict([x_s2_feature_vector])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uim77OYgFkil"
   },
   "source": [
    "Ok, does this looks like a reasonable prediction? Let's again try to build an intuition of the prediction derived from the SVM model $h$ based on the distinct iris flower class distributions including $x^{s2}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhqxEB_UFkil"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observations to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([[7.8, 2.3, 6.4, 2.50, \"observation s2\"]], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kme8FqflFkil"
   },
   "source": [
    "Ok, the feature distributions of the feature values observable for the unknown iris flower $x^{s1}$ exhibit a high likelihood of beeing of class **virginica**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1YfNlHYFkim"
   },
   "source": [
    "### 5.2 Linear Support Vector Machine (SVM) Classifers - The Non-Linear Seperable Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A_xLlIGFkim"
   },
   "source": [
    "Ok, great we have seen how to apply Support Vector classification to separable data. So how can we extend these ideas to handle non-separable data? To achieve this we would like to relax the initial constraints $ x_{i} \\cdot w + b \\geq + 1, y_{i} = +1 $ and $ x_{i} \\cdot w + b \\leq - 1, y_{i} = -1 $ when necessary. That is, we would like to introduce a further cost for doing so. This can be done by the introducing of so-called positive **\"slack variables\"** denoted $\\xi_{i}, i=1, ..., l$ in the Lagrange optimization $L_{P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r45RamaMFkim"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"hyperplane_slack.png\">\n",
    "\n",
    "Linear separating hyperplanes $H_{1}$, $H_{2}$, and $H^{*}$ for the non-separable case. The support vectors that constitute $H_{1}$, $H_{2}$ are circled.\n",
    "\n",
    "(Source: https://link.springer.com/article/10.1023/A:1009715923555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19ujjmKFkim"
   },
   "source": [
    "Therefore, the initial constraints become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1nM64NOFkim"
   },
   "source": [
    "$$ x_{i} \\cdot w + b \\geq + 1 - \\xi_{i}, y_{i} = +1 $$\n",
    "\n",
    "$$ x_{i} \\cdot w + b \\leq - 1 + \\xi_{i}, y_{i} = -1 $$\n",
    "\n",
    "$$ \\xi_{i} \\geq 0,  \\forall i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoBTvG3TFkim"
   },
   "source": [
    "Thus, for an error to occur, the corresponding $\\xi_{i}$ must exceed unity. As a result, $\\sum_{i=1}^{l} \\xi_{i}$ defines an upper bound on the number of training errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90-u_I-NFkim"
   },
   "source": [
    "#### 5.2.1 A \"Primal\"  Optimization Objective Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNPo1mJYFkis"
   },
   "source": [
    "A natural way to assign such an extra cost for errors is to add it to the primal Lagrangian objective function $L_{P}$ to be optimized. The Lagrangian therefore becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP15n4VpFkit"
   },
   "source": [
    "$$\\underset{w, \\alpha}{\\arg \\min} \\; L_{P} = \\frac{1}{2}||w||^{2} + C \\sum_{i=1}^{l} \\xi_{i} - \\sum_{i=1}^{l} \\alpha_{i}\\{y_{i}(x_{i} \\cdot w + b) -1 + \\xi_{i}\\} + \\sum_{i=1}^{l}\\alpha_{i} - \\sum_{i=1}^{l} \\mu_{i} \\xi_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9bKl_ayFkit"
   },
   "source": [
    "where $C$ is a parameter determines the penalty magnitude of errors. Furthermore, $\\mu_{i}$ are another set of Lagrange multipliers introduced to enforce positivity of the slack variables $\\xi_{i}$. We must now minimize $L_{P}$ with respect to $w$, $b$. Thereby, \n",
    "\n",
    "> 1. the minimization of the first term $\\frac{1}{2}||w||^{2}$ maximizes the margin of the separating hyperplane,\n",
    "> 2. the minimization of the second term $C \\sum_{i=1}^{l} \\xi_{i}$ minimizes the penalty of misclassfied training samples,\n",
    "> 3. the maximization of the third term $\\sum_{i=1}^{l} \\alpha_{i}y_{i}(x_{i} \\cdot w + b)$ maximizes the number of correctly classfied training samples,\n",
    "> 4. the minimization of the fourth term $\\sum_{i=1}^{l}\\alpha_{i}$ minimizes the number of support vectors, \n",
    "> 5. the maximization of the fifth term $\\sum_{i=1}^{l} \\mu_{i} \\xi_{i}$ enforces the positivity of the slack variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze1X2A4LFkiu"
   },
   "source": [
    "In general, the penalty term $C$ is a parameter to be chosen by the user. A larger $C$ corresponds to assigning a higher penalty to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMCPevemFkiu"
   },
   "source": [
    "#### 5.2.2 A \"Dual\" Optimization Objective Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLX_VISLFkiu"
   },
   "source": [
    "We can again derive a dual formulation of the optimization objective using the conditions that $w = \\sum_{i=1}^{l} \\alpha_{i}y_{i}x_{i}$ and $\\sum_{i=1}^{l}\\alpha_{i}y_{i} = 0$, which becomes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCtQjbRZFkiu"
   },
   "source": [
    "$$\\underset{\\alpha}{\\arg \\min} \\; L_{D} = \\sum_{i=1}^{l}\\alpha_{i} + \\frac{1}{2} \\sum_{i,j=1}^{l} \\alpha_{i}\\alpha_{j}y_{i}y_{j}<x_{i}, x_{j}>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzTcEGjxFkiv"
   },
   "source": [
    "subject to $0 \\leq \\alpha_{i} \\leq C$. The only difference in comparison to the optimal hyperplane case is that the $\\alpha_{i}$ now have an upper bound of C. Again, the optimal seperating hyperplane $H^{*}$ still remains a linear function of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv_EG20eFkiw"
   },
   "source": [
    "#### 5.2.3 Training of a Support Vector Machine (SVM) Classifier Using Different C Parameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PMxVhl5Fkiw"
   },
   "source": [
    "Let's inspect different parametrizations of $C$ and their corresponding impact on the determined support vectors and learned optimal separating hyperplane $H^{*}$. We can obtain the learned support vectors from the model using the `support_vectors_` method available `Scikit-Learn`. Let's again fit a linear SVM to the training data observations $x_{i}$ using a penalty of $C=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXX5b5PtFkiw"
   },
   "outputs": [],
   "source": [
    "# init the Support Vector Machine classifier\n",
    "svm = SVC(kernel='linear', C=1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB8yItmjFkix"
   },
   "source": [
    "We will train the SVM model on the **sepal length** $x_1$ and **petal length** $x_3$ features of the iris flower dataset to separate flowers of the classes $c_{1}=$ **versicolor** and $c_{2}=$ **virginica**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R5UmidDFkix"
   },
   "outputs": [],
   "source": [
    "x_train_test = x_train[y_train != 0, :][:, [0,2]]\n",
    "y_train_test = y_train[y_train != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4J8-L4JFkiy"
   },
   "source": [
    "Let's fit the linear SVM model using the penalty of $C=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zIKKMI_Fkiy"
   },
   "outputs": [],
   "source": [
    "svm.fit(x_train_test, y_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3KTdsITFkiy"
   },
   "source": [
    "Let's briefly glance over the determined **support vectors for which $\\alpha_{i} > 0$** and that **constitute the learned max-margin separating hyperplane $H^{*}$**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2q1A1rQuFkiz"
   },
   "outputs": [],
   "source": [
    "svm.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20pfJwTSFkiz"
   },
   "source": [
    "Finally, let's visually inspect the **maximum margin separating hyperplane $H^{*}$** that was learned by our SVM. Remember, the learned hyperplane was optimized to seperate the features sepal length $x_1$ and petal length $x_3$ of the iris flower classes $c_{1}=$ versicolor and $c_{2}=$ virginica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCukuC13Fki0"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot sepal length vs. petal length and corresponding classes\n",
    "ax.scatter(x_train[:,0], x_train[:,2], c=y_train, cmap=plt.cm.Set1)\n",
    "\n",
    "# highlight the determined support vectors in green\n",
    "ax.scatter(svm.support_vectors_[:,0], svm.support_vectors_[:,1], s=200, linewidth=1, facecolor='none', edgecolors='k', label='support vectors')\n",
    "\n",
    "# determine axis ranges\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create meshgrid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# determine and plot decision boundary\n",
    "Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[sepal_length]\", fontsize=14)\n",
    "ax.set_ylabel(\"[petal_length]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Sepal Length vs. Petal Length - Decision Boundary', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yry3xH1RFki0"
   },
   "source": [
    "Ok, we can observe how the learned **24 support vectors** nicely constitute the optimal maximum margin separating hyperplane $H^{*}$. Let's now investigate how different values of $C \\in \\{0.1, 10, 100, 1000\\}$ will penalize and therefore affect the number of support vectors. Remember, a larger value of $C$ corresponds to assigning a higher penalty to errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ntxwxek4Fki0"
   },
   "outputs": [],
   "source": [
    "# init distinct C values\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "# init SVM models of distinct C values\n",
    "svm_models = (SVC(kernel='linear', C=C, random_state=random_seed) for C in C_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX2vepYCFki1"
   },
   "source": [
    "Let's fit the linear SVM models using distinct values of the penalty term $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNuZJMsvFki1"
   },
   "outputs": [],
   "source": [
    "# fit the distinct SVM models to the data\n",
    "svm_models = (model.fit(x_train_test, y_train_test) for model in svm_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO-yJ3bKFki1"
   },
   "source": [
    "Let's now again visually inspect the maximum margin separating hyperplane $H^{*}$ that was learned by our SVM and applying different values of $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMLxmAMIFki1"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig, sub = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "# iterate over distinct models\n",
    "for model, ax in zip(svm_models, sub.flatten()):\n",
    "    \n",
    "    # add grid\n",
    "    ax.grid(linestyle='dotted')\n",
    "\n",
    "    # plot sepal length vs. petal length and corresponding classes\n",
    "    ax.scatter(x_train[:,0], x_train[:,2], c=y_train, cmap=plt.cm.Set1)\n",
    "\n",
    "    # highlight the determined support vectors in green\n",
    "    ax.scatter(model.support_vectors_[:,0], model.support_vectors_[:,1], s=200, linewidth=1, facecolor='none', edgecolors='k', label='support vectors')\n",
    "\n",
    "    # determine and plot decision boundary\n",
    "    Z = model.decision_function(xy).reshape(XX.shape)\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "    # add axis legends\n",
    "    ax.set_xlabel(\"[sepal_length]\", fontsize=14)\n",
    "    ax.set_ylabel(\"[petal_length]\", fontsize=14)\n",
    "\n",
    "    # add plot title\n",
    "    ax.set_title('Decision Boundary, C={}, kernel=\\'{}\\''.format(str(model.C), str(model.kernel)), fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndxjJBMNFki2"
   },
   "source": [
    "We can indeed observe that with increasing $C$ the number of misclassifications as well as the number of support vectors that constitute $H^{*}$ decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEFl1iIOFki2"
   },
   "source": [
    "#### 5.2.4 Non-Linear Support Vector Machine (SVM) Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXnhLthwFki2"
   },
   "source": [
    "How can the above linear SVMs be generalised to the case where the optimal separating hyperplane $H^{*}$ can not be formulated as a linear function of the data? This holds for instances when the training data is not linearly separable. Boser, Guyon and Vapnik [7] showed the so-called **\"kernel trick\"** (introduced by Aizermann[8]) could be used to accomplish this in a surprisingly straightforward way. First notice again, from the training objectives dual formulation, that the only way in which the data appears in the objective is in the form of dot products $<x_{i}, x_{j}>$. Now suppose we first mapped the data to some other (possibly infinite-dimensional) Euclidean space $\\mathcal{H}$, using the mapping which we will call $\\phi$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE9_XJo0Fki2"
   },
   "source": [
    "$$\\phi: \\mathcal{R}^{d} \\mapsto \\mathcal{H}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvJiYkDAFki2"
   },
   "source": [
    "Then, of course, the training algorithm would only depend on the data through dot products in $\\mathcal{H}$, i.e. on functions of the form $\\phi(x_{i}) \\cdot \\phi(x_{j})$. Now if there were a **\"kernel function\"** $K$ such that $K(x_{i}, x_{j}) = \\phi(x_{i}) \\cdot \\phi(x_{j})$, we would only need to use $K$ in the training algorithm, and would never need to explicitly even know what $\\phi$ is. One such kernel function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohVdFCSgFki2"
   },
   "source": [
    "$$K(x_{i}, x_{j}) = e^{-||x_{i}-x_{j}||^{2} / 2 \\sigma^{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8In_rrfFki3"
   },
   "source": [
    "In this particular example, $\\mathcal{H}$ is infinite-dimensional, so it would not be very easy to work with $\\phi$ explicitly. However, if one replaces $x_{i} \\cdot x_{j}$ by $K(x_{i}, x_{j})$ everywhere in the training procedure, the algorithm will happily produce a SVM which lives in an infinite-dimensional space. All considerations of the previous sections still hold, since we are still doing a linear separation but in a different space. Since we can again derive a dual formulation of the optimisation objective using the conditions that $w = \\sum_{i=1}^{l} \\alpha_{i}y_{i}x_{i}$ and $\\sum_{i=1}^{l}\\alpha_{i}y_{i} = 0$, which becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CbYhbQKFki3"
   },
   "source": [
    "$$\\underset{\\alpha}{\\arg \\min} \\; L_{D} = \\sum_{i=1}^{l}\\alpha_{i} + \\frac{1}{2} \\sum_{i,j=1}^{l} \\alpha_{i}\\alpha_{j}y_{i}y_{j}K(x_{i}, x_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpxCkHQYFki3"
   },
   "source": [
    "subject to $0 \\leq \\alpha_{i} \\leq C$. The only difference in comparison to the linear hyperplane case is that the dot product $<x_{i}, x_{j}>$ is now replaced by a kernel function $K(x_{i}, x_{j})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DQP-2rFki3"
   },
   "source": [
    "#### 5.2.5 Training of a Support Vector Machine (SVM) Classifier Using Different Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HHZPjmFFki3"
   },
   "source": [
    "Let's now train a set of non-linear SVMs and evaluate different kernel functions $K(x_{i}, x_{j})$. We will again train the distinct SVM models on the sepal length $x_1$ and petal length $x_3$ features of the iris flower dataset to separate the distinct flower classes $c_{0}=$ setosa, $c_{1}=$ versicolor and $c_{2}=$ virginica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cGAqFzHFki4"
   },
   "outputs": [],
   "source": [
    "x_train_kernel = x_train[:, [0, 2]]\n",
    "y_train_kernel = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Kd3JD7Fki4"
   },
   "source": [
    "Next, we will instantiate several SVM models each equipped with a different kernel function. Thereby, we will use three of the kernel functions already available in the `Scikit-Learn` library: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmd5TWwpFki4"
   },
   "source": [
    "> 1. linear kernel function: **$<x_{i}, x_{j}>$**,\n",
    "> 2. radial-basis kernel-function: $exp({- \\gamma ||x_{i}, x_{j}||^{2}})$, where $\\gamma$ is specified by the keyword `gamma` and must be greater than 0,\n",
    "> 3. polynomial kernel-function: $(\\gamma <x_{i}, x_{j}> + r)^{d}$, where $d$ is specified by the keyword `degree` and $r$ by `coef0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUPDdZcsFki4"
   },
   "source": [
    "Let's instantiate the distinct SVM models accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbTjWC57Fki5"
   },
   "outputs": [],
   "source": [
    "# init the SVM models using distinct kernel functions\n",
    "svm_models = (SVC(kernel='linear', C=1)\n",
    "              , SVC(kernel='rbf', gamma=0.1, C=1)\n",
    "              , SVC(kernel='rbf', gamma=0.2, C=1)\n",
    "              , SVC(kernel='rbf', gamma=0.5, C=1)\n",
    "              , SVC(kernel='rbf', gamma=0.7, C=1)\n",
    "              , SVC(kernel='poly', degree=1, coef0=1.0, C=1)\n",
    "              , SVC(kernel='poly', degree=2, coef0=1.0, C=1)\n",
    "              , SVC(kernel='poly', degree=5, coef0=1.0, C=1)\n",
    "              , SVC(kernel='poly', degree=7, coef0=1.0, C=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v_VRMhvFki5"
   },
   "source": [
    "Let's subsequently train the distinct SVM models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-myyDnAFki5"
   },
   "outputs": [],
   "source": [
    "# fit the distinct SVM models to the data\n",
    "svm_models = (model.fit(x_train_kernel, y_train_kernel) for model in svm_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKQ7ioOdFki5"
   },
   "source": [
    "Let's visually inspect the optimal separating hyperplane $H^{*}$ learned by the distinct kernel functions $K(x_{i}, x_{j})$ to separate the sepal length $x_1$ and petal length $x_3$ features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecA9RiluFki5"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "fig, sub = plt.subplots(3, 3, figsize=(14, 14))\n",
    "\n",
    "# determine mesh-grid limitations\n",
    "xlim = [np.min(x_train[:, 0]) - 0.8, np.max(x_train[:, 0]) + 0.8]\n",
    "ylim = [np.min(x_train[:, 2]) - 0.8, np.max(x_train[:, 2]) + 0.8]\n",
    "\n",
    "# create meshgrid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 1000)\n",
    "yy = np.linspace(ylim[0], ylim[1], 1000)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# iterate over distinct models\n",
    "for model, ax in zip(svm_models, sub.flatten()):\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    # add grid\n",
    "    ax.grid(linestyle='dotted')\n",
    "    \n",
    "    Z = model.predict(xy).reshape(XX.shape)\n",
    "    ax.contourf(XX, YY, Z, alpha=0.5, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # plot sepal length vs. petal length and corresponding classes\n",
    "    ax.scatter(x_train[:,0], x_train[:,2], c=y_train, cmap=plt.cm.Set1)\n",
    "\n",
    "    # highlight the determined support vectors in green\n",
    "    ax.scatter(model.support_vectors_[:,0], model.support_vectors_[:,1], s=200, linewidth=1, facecolor='none', edgecolors='k', label='support vectors')\n",
    "    \n",
    "    # set axis ranges\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    # add axis legends\n",
    "    ax.set_xlabel('[sepal_length]', fontsize=10)\n",
    "    ax.set_ylabel('[petal_length]', fontsize=10)\n",
    "    \n",
    "    # add plot title\n",
    "    ax.set_title('C={}, kernel=\\'{}\\', degree=\\'{}\\', gamma=\\'{}\\''.format(str(model.C), str(model.kernel), str(model.degree), str(model.gamma)), fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABKY_ZzVFkjN"
   },
   "source": [
    "### 5.3 References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7kOEua8FkjO"
   },
   "source": [
    "[1] **\"A Tutorial on Support Vector Machines for Pattern Recognition\"**, Burges C. J. C. , Bell Laboratories, Lucent Technologies, Data Mining and Knowledge Discovery, 2, 121-167, 1998.\n",
    "\n",
    "[2] **\"Support Vector Networks\"**, Cortes C. and Vapnik V., Machine Learning, 20:273-297, 1995. \n",
    "\n",
    "[3] **\"Comparison of View-Based Object Recognition Algorithms Using Realistic 3D Models\"**, Blanz V., Schölkopf B., Bülthoff H., Burges C., Vapnik V., and Vetter T., Artificial Neural Networks - ICANN'96, 251-256, 1996. \n",
    "\n",
    "[4] **\"Identifying Speaker with Support Vector Networks\"**, Schmidt M., Interface '96 Proceedings, 1996. \n",
    "\n",
    "[5] **\"Training Support Vector Machines: An Application to Face Detection\"**, Osuna E., Freund R., Girosi F., IEEE Conference on Computer Vision and Pattern Recognition, 130-136, 1997.\n",
    "\n",
    "[6] **\"Text Categorization With Support Vector Machines\"**, Joachims T., Technical Report, LS VIII Number 23, University of Dortmund, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n94u0rxat8su"
   },
   "source": [
    "## 6. Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCOEZj-it8sv"
   },
   "source": [
    "In this lab, a step by step introduction into (1) Gaussian **Naive-Bayes (NB)** classification and (2) **Support Vector Machine** classification is presented. The code and exercises presented in this lab may serve as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzWZDKpat8s4"
   },
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Please note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D-azATot8s4"
   },
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z07F-kot8s_"
   },
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4Z4L4vIt8s_"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script lab_03.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "D0Jnx-Ljt8lK",
    "CZaa0qAnt8lY",
    "E5MbyOLHVzo5",
    "mMSfpCPvt8l4",
    "g_6MsT_JYZLu",
    "n94u0rxat8su"
   ],
   "name": "lab_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "331px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
