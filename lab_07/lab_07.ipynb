{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import copy\n","import time\n","import re\n","import string\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n","from unicodedata import bidirectional\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, pad_sequence, pad_packed_sequence\n","from torch import nn\n","import torch\n","import torchtext\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentiment_dict = {\n","    0: 'negative',\n","    1: 'somewhat negative',\n","    2: 'neutral',\n","    3: 'somewhat positive',\n","    4: 'positive'\n","}\n","def load_sentiment_data(path='res/train.tsv'):\n","    df = pd.read_csv(path, sep='\\t', header=0)\n","    #    columns = ['PhraseId' 'SentenceId', 'Phrase', 'Sentiment']\n","    def process_phrase(phrase):\n","        remove_pun = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n","        phrase = phrase.translate(remove_digits)\n","        phrase = phrase.translate(remove_pun)\n","        phrase = re.sub(' {2,}', ' ', phrase)\n","        return phrase.lower()\n","    df['Phrase'] = df['Phrase'].apply(lambda x: process_phrase(x))\n","    # filter out empty phrases\n","    df = df[df['Phrase'].str.len() > 1]\n","    df = df.reset_index(drop=True)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The first time you run this will download a ~823MB file\n","glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n","# the glove object acts as the vocabulary here, can look up lower case words, check this whenb prprocessing\n","# glove has stoi (string ot index)\n","# glove has itos, a list of token string index by their numerical identifiers)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class RottenTomatoesDataset(Dataset):\n","    def __init__(self, df, glove_vocab, label_col='Sentiment', unk='<unk>') -> None:\n","        super().__init__()\n","        self.df = df\n","        self.labels = self.df[label_col].values\n","        self.glove = glove_vocab\n","        self.vocab_size = len(glove_vocab)\n","        self.data = []\n","\n","        for title in self.df['Phrase'].values:\n","            self.data.append(torch.stack(\n","                [torch.LongTensor([glove.stoi.get(w, glove.stoi.get(unk))]) for w in title.split()]))\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        return self.data[idx], self.labels[idx]\n","\n","class SequencePadder():\n","    def __init__(self, symbol) -> None:\n","        self.symbol = symbol\n","\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n","        sequences = [x[0] for x in sorted_batch]\n","        labels = [x[1] for x in sorted_batch]\n","        padded = pad_sequence(sequences, padding_value=self.symbol)\n","        lengths = torch.LongTensor([len(x) for x in sequences])\n","        return padded, torch.LongTensor(labels), lengths\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_metrics(model, data_loader, device, get_sentences=False):\n","    # use with batch size 1!\n","    with torch.set_grad_enabled(False):\n","        model.eval()\n","        model.to(device)\n","        y_pred, y_true = [], []\n","        sentences = []\n","        for inputs, labels, lengths in data_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            out, weights = model(inputs, lengths)\n","\n","            _, preds = torch.max(out, 1)\n","\n","            y_pred.append(preds)\n","            y_true.append(labels)\n","            if get_sentences:\n","                sent = [data_loader.dataset.glove.itos[i.item()] for i in inputs]\n","                sentences.append((sent, weights))\n","        if get_sentences:\n","            return scores, sentences\n","        else:\n","            return {\n","                      'f1': f1_score(y_true, y_pred, average='micro'),\n","                      'prec': precision_score(y_true, y_pred, average='micro'),\n","                      'recall': recall_score(y_true, y_pred, average='micro'),\n","                      'acc': accuracy_score(y_true, y_pred),\n","                   }\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LstmClassifierGloveEmbeddings(nn.Module):\n","    def __init__(self,\n","                hidden_size,\n","                output_size, # number of classes\n","                glove=None,\n","                num_layers=1,\n","                bidirectional=False):\n","\n","        super(LstmClassifierGloveEmbeddings, self).__init__()\n","        self.input_size = len(glove) # vocabulary size\n","        self.embedding_size = glove.dim\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","\n","        # nn.Embedding can also be used with your own embeddings\n","        # hint: if you want to do so, you need to adapt the Dataloader\n","        self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n","\n","        self.lstm = nn.LSTM(\n","                        input_size=self.embedding_size,\n","                        hidden_size=hidden_size,\n","                        num_layers=self.num_layers,\n","                        dropout=0.2 if num_layers > 1 else 0,\n","                        bidirectional=bidirectional,\n","        )\n","\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        fc_size = self.hidden_size * self.num_directions\n","        self.fc = nn.Linear(fc_size, output_size)\n","\n","    def forward(self, x, lengths, h_n=None):\n","        if h_n is None:\n","            h_n, c_n = self.init_hidden(x.size(1))\n","        else:\n","            h_n =  h_n[0]\n","            c_n =  h_n[1]\n","        # seq_len, batch_size = x.size()\n","        embed = self.embedding(x).squeeze(2)\n","        packed_seq = pack_padded_sequence(embed, lengths)\n","        # packed squence helps avoid unneccsary computation, with the length it marks out irrelvant/ padded sequence\n","        # elements, this allows the efficient computation of sequences of different lengths inside the same batch\n","        out, (h_n, cn) = self.lstm(packed_seq, (h_n, c_n))\n","        # out containing the output features (h_t) from the last layer of the LSTM, for each t. I\n","        # h_n containing the final hidden state for each element in the batch.\n","        # c_n containing the final cell state for each element in the batch.\n","        # output.view(seq_len, batch, num_directions, hidden_size)\n","\n","        # padded_seq, lens = pad_packed_sequence(out) # undoing pack_padded_sequnce, not necessary here\n","        # h_n.view(num_layers, num_directions, batch, hidden_size) # addressable per layer\n","        if self.num_directions == 2:\n","            h_forward_backward = h_n.view(2, 2, x.size(1), -1)[-1]\n","            h_forward_backward = torch.cat([h_forward_backward[0], h_forward_backward[1]], 1)\n","            logits = self.fc(h_forward_backward) #h only hidden state at last layer, if bidrect out[-1 contains the concatenated hidden state]\n","        else:\n","            logits = self.fc(h_n) # h_n only hidden state at last layer, if bidrect out[-1 contains the concatenated hidden state]\n","        # dont use batch first here, seq_len must be first dimension\n","\n","        # k-vectors are: quer output gru, query vector:\n","        return logits, h_n # only hidden state for the last layer is needed for loss calculation\n","\n","    def init_hidden(self, batch_size=1):\n","        # if you want zero init, this does not have to be done manually in newer versions of pytorch\n","        # https://discuss.pytorch.org/t/lstm-hidden-state-changing-dimensions-error/23359\n","        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","        # h_0 of shape (num_layers * num_directions, batch, hidden_size)\n","        h_dim_0 = self.num_layers * self.num_directions\n","        hidden = (torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device),\n","                  torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device))\n","\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LstmSelfAttentionGloveEmbeddings(nn.Module):\n","    def __init__(self,\n","                hidden_size,\n","                output_size, # number of classes\n","                glove=None,\n","                num_layers=1,\n","                bidirectional=False):\n","\n","        super(LstmSelfAttentionGloveEmbeddings, self).__init__()\n","        self.input_size = len(glove) # vocabulary size\n","        self.embedding_size = glove.dim\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","\n","        # nn.Embedding can also be used with your own embeddings\n","        # hint: if you want to do so, you need to adapt the Dataloader\n","        self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n","\n","        self.lstm = nn.LSTM(\n","                        input_size=self.embedding_size,\n","                        hidden_size=hidden_size,\n","                        num_layers=self.num_layers,\n","                        dropout=0.2 if num_layers > 1 else 0,\n","                        bidirectional=bidirectional,\n","        )\n","\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        self.attention = nn.MultiheadAttention(hidden_size * self.num_directions,\n","                                               1, dropout=0.2)\n","\n","        fc_size = self.hidden_size * self.num_directions\n","        self.fc = nn.Linear(fc_size, output_size)\n","\n","    def forward(self, x, lengths, h_n=None):\n","        if h_n is None:\n","            h_n, c_n = self.init_hidden(x.size(1))\n","        else:\n","            h_n =  h_n[0]\n","            c_n =  h_n[1]\n","        # seq_len, batch_size = x.size()\n","        embed = self.embedding(x).squeeze(2)\n","        packed_seq = pack_padded_sequence(embed, lengths)\n","        # packed squence helps avoid unneccsary computation, with the length it marks out irrelvant/ padded sequence\n","        # elements, this allows the efficient computation of sequences of different lengths inside the same batch\n","        out, (h_n, cn) = self.lstm(packed_seq, (h_n, c_n))\n","    \n","        padded_seq, lens = pad_packed_sequence(out) # undoing pack_padded_sequnce, not necessary here\n","        # self attention sees q,k and v to be from the same input, in our case all sequential hidden states\n","        # values retrieved are in the dimensionality of hidden state in this case\n","        # dont use batch first here, seq_len must be first dimension\n","        attn_output, attn_weights = self.attention(padded_seq, padded_seq, padded_seq)\n","\n","        weighted_mean = attn_output.mean(0) # mean of attention values\n","        # this makes it the attention weighted mean of the hidden states\n","        logits = self.fc(weighted_mean)\n","        # k-vectors are: quer output gru, query vector:\n","        return logits, attn_weights # only hidden state for the last layer is needed for loss calculation\n","\n","    def init_hidden(self, batch_size=1):\n","        # if you want zero init, this does not have to be done manually in newer versions of pytorch\n","        # https://discuss.pytorch.org/t/lstm-hidden-state-changing-dimensions-error/23359\n","        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","        # h_0 of shape (num_layers * num_directions, batch, hidden_size)\n","        h_dim_0 = self.num_layers * self.num_directions\n","        hidden = (torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device),\n","                  torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device))\n","\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_rnn_model(model, data_loaders, criterion, optimizer, device, num_epochs=25):\n","    '''\n","    @param: data_loaders: takes on data loader containing the test set and one containing the train set\n","            keys must be: 'train', 'test' in this case\n","    '''\n","    since = time.time()\n","    # best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch = 0\n","    best_acc = 0.0\n","    test_losses = []\n","    print(model)\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(1, num_epochs + 1):\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","        print('-' * 10)\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'test']:\n","            is_train = phase == 'train'\n","            if is_train:   # set model mode\n","                model.train()\n","            else:\n","                model.eval()\n","            running_loss = 0.0\n","            running_corrects = 0\n","            # Iterate over data in DataLoaders\n","            for inputs, labels, lens in data_loaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                # model = model.to(device) model should already be @device\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(is_train):\n","                    out, h_n = model(inputs, lens) \n","                    # take only the last output\n","                    _, preds = torch.max(out, 1)\n","                    loss = criterion(out, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if is_train:\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(1)\n","                running_corrects += torch.sum(preds == labels.data)\n","            # if is_train:\n","                # scheduler.step()\n","\n","            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(data_loaders[phase].dataset)\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                                            phase, epoch_loss, epoch_acc))\n","            if not is_train:\n","                test_losses.append(epoch_loss)\n","            if (not is_train) and epoch_loss <= min(test_losses):\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_epoch = epoch\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best test Acc: {:4f}'.format(best_acc))\n","    print('Best test loss: {:4f}'.format(min(test_losses)))\n","    model.load_state_dict(best_model_wts)\n","    return model, f'{best_acc:3.4f}', best_epoch\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mylstm = LstmClassifierGloveEmbeddings(100, len(sentiment_dict), glove, num_layers=2, bidirectional=True)\n","print(mylstm)\n","out, h_n = mylstm(torch.stack([torch.LongTensor([3]), torch.LongTensor([1])]), [1])\n","torch.stack([torch.LongTensor(1), torch.LongTensor(1)]).shape\n","print(out.shape) # (L, N, D*H_out)\n","print(h_n.shape) # final hidden state, (D * num_layers, N, H_out) final hidden state for each element in the batch\n","# N = batch_size\n","# L = sequence_length\n","# D = 2 if bidrecitional esle 1\n","# H_in = input size\n","# H_cell = hidden_size\n","# H_out = proj_size if proj_size > 0 else hidden_size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["myatt = LstmSelfAttentionGloveEmbeddings(100, len(sentiment_dict), glove, num_layers=2, bidirectional=True)\n","print(mylstm)\n","out, weights = myatt(torch.stack([torch.LongTensor([3]), torch.LongTensor([1])]), [1])\n","torch.stack([torch.LongTensor(1), torch.LongTensor(1)]).shape\n","print(out.shape) # (L, N, D*H_out)\n","print(h_n.shape) # final hidden state, (D * num_layers, N, H_out) final hidden state for each element in the batch\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_epochs = 50 #00\n","hidden_size = 64\n","n_layers = 2\n","batch_size = 8\n","bi_direct = True\n","lr = 0.0001\n","shuffle = True\n","debug = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# prep data\n","def append_special(glove, special, vec=None):\n","    glove.itos.append(special)\n","    glove.stoi[special] = glove.itos.index(special)\n","    if vec is None:\n","        vec = torch.zeros(1, glove.vectors.size(1))\n","    glove.vectors = torch.cat((glove.vectors, vec))\n","    return glove\n","\n","pad_sym = '<pad>'\n","unk = '<unk>'\n","df= load_sentiment_data()\n","glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n","glove = append_special(glove, unk)\n","glove = append_special(glove, pad_sym)\n","print(df.Sentiment.value_counts())\n","if debug:\n","    df = df.iloc[0:2500]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training of 'normal' classifier with lstm-> sentiment encoded to last hidden state, FC layer to classify\n","# 5 fold cv mindful of data distribution\n","# cpu training 1 fold took ~ 3hours on my machine(macbook pro 2017, i7, 16GB RAM),\n","# best acc 65.78 / test loss 0.839\n","kf = StratifiedKFold(n_splits=5, shuffle=True)\n","results_dict = {}\n","fold = 1\n","for train, test in kf.split(df.index, df['Sentiment']):\n","    datasets = ['train', 'test']\n","    model = LstmClassifierGloveEmbeddings(\n","                                          hidden_size,\n","                                          output_size=len(df['Sentiment'].value_counts()),\n","                                          glove=glove,\n","                                          num_layers=n_layers,\n","                                          bidirectional=bi_direct,)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","    train_ds = RottenTomatoesDataset(df.iloc[train], glove)\n","    test_ds = RottenTomatoesDataset(df.iloc[test], glove)\n","\n","    data_loaders = {\n","        'train': DataLoader(train_ds, drop_last=True,\n","                            collate_fn=SequencePadder(glove.stoi[pad_sym]),\n","                            batch_size=batch_size, shuffle=shuffle),\n","        'test': DataLoader(test_ds, batch_size=batch_size, drop_last=shuffle,\n","                            collate_fn=SequencePadder(glove.stoi[pad_sym]))\n","                }\n","    # train model returns the best model for the current run\n","    model, acc, best_epoch = train_rnn_model(model, data_loaders, criterion, optimizer,\n","                                             device, num_epochs=n_epochs)\n","    # run eval for best model and save for this split\n","    scores = get_metrics(model, DataLoader(RottenTomatoesDataset(df.iloc[test], glove),\n","                                            collate_fn=SequencePadder(glove.stoi[pad_sym]), \n","                                            batch_size=1), device)\n","    # results_dict[fold] = scores\n","    fold += 1\n","    # print(scores)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pd.DataFrame(results_dict).T.mean())\n","print(pd.DataFrame(results_dict).T.std())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# mean metric results for 5 folds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training With attention model needs a slightly modified loop, params come out differently\n","def train_lstm_att_model(model, data_loaders, criterion, optimizer, device, num_epochs=25):\n","    '''\n","    @param: data_loaders: takes on data loader containing the test set and one containing the train set\n","            keys must be: 'train', 'test' in this case\n","    '''\n","    since = time.time()\n","    best_epoch = 0\n","    best_acc = 0.0\n","    test_losses = []\n","    print(model)\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(1, num_epochs + 1):\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","        print('-' * 10)\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'test']:\n","            is_train = phase == 'train'\n","            if is_train:   # set model mode\n","                model.train()\n","            else:\n","                model.eval()\n","            running_loss = 0.0\n","            running_corrects = 0\n","            for inputs, labels, lens in data_loaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                with torch.set_grad_enabled(is_train):\n","                    out, weights = model(inputs, lens)\n","                    # take only the last output\n","                    _, preds = torch.max(out, 1)\n","                    loss = criterion(out, labels)\n","\n","                    if is_train:\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(1)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(data_loaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(data_loaders[phase].dataset)\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                                            phase, epoch_loss, epoch_acc))\n","            if not is_train:\n","                test_losses.append(epoch_loss)\n","            if (not is_train) and epoch_loss <= min(test_losses):\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_epoch = epoch\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best test Acc: {:4f}'.format(best_acc))\n","    print('Best test loss: {:4f}'.format(min(test_losses)))\n","    model.load_state_dict(best_model_wts)\n","    return model, f'{best_acc:3.4f}', best_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kf = StratifiedKFold(n_splits=5, shuffle=True)\n","results_dict = {}\n","fold = 1\n","for train, test in kf.split(df.index, df['Sentiment']):\n","    datasets = ['train', 'test']\n","    model = LstmSelfAttentionGloveEmbeddings(\n","                                          hidden_size,\n","                                          output_size=len(df['Sentiment'].value_counts()),\n","                                          glove=glove,\n","                                          num_layers=n_layers,\n","                                          bidirectional=bi_direct,)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","    train_ds = RottenTomatoesDataset(df.iloc[train], glove)\n","    test_ds = RottenTomatoesDataset(df.iloc[test], glove)\n","\n","    data_loaders = {\n","        'train': DataLoader(train_ds, drop_last=True,\n","                            collate_fn=SequencePadder(glove.stoi[pad_sym]),\n","                            batch_size=batch_size, shuffle=shuffle),\n","        'test': DataLoader(test_ds, batch_size=batch_size, drop_last=shuffle,\n","                            collate_fn=SequencePadder(glove.stoi[pad_sym]))\n","                }\n","    # train_lstm_att_model returns the best model for the current run\n","    model, acc, best_epoch = train_lstm_att_model(model, data_loaders, criterion, optimizer,\n","                                                 device, num_epochs=n_epochs)\n","    # run eval for best model and save for this split\n","    scores = get_metrics(model, DataLoader(RottenTomatoesDataset(df.iloc[test], glove),\n","                                            collate_fn=SequencePadder(glove.stoi[pad_sym]), \n","                                            batch_size=1), device)\n","    # results_dict[fold] = scores\n","    fold += 1\n","    # print(scores)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(pd.DataFrame(results_dict).T.mean())\n","print(pd.DataFrame(results_dict).T.std())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}